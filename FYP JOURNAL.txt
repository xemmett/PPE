3 options 03/10/2021:

- cv (computer vision, Tesseract): taking images of what the bot sees
converting the image into a timetable using tabular, scrapy or with PIL

issue with tabular: tabular acts funky when you have 2 pages, it realies heavily on inputted
tables to be fully bordered, otherwise it can read them both into two different dataframes, casuing double the work.

issue with Tesseract:
PIL can be used to change the images gradient to make edges and colors to standout making it easier to read, Tesseract is a cv tool to read text from images, it would be a lot of text
data processing unless we also used PILS with Tesseract to define borders and go cell by cell to read information.

^^ these are all emulations of human eyes and brains, we scour, we scan, we take information and go.

Scrapy:
Scrapy is realistically the best, basic HTML web scraping, I do believe though, in future we will not be allowed access to the HTML of a website. Every day it becomes harder and harder.
Human eye and computer processing is what the future of rapid web scraping will be. As tesseract evolves and more tools are built for these open source powerful tools.


Completed 04/10/2021:
- qualifax_course_scraper.py finished.
- spoke to denis moore about setting up my raspberry pi in the server room.

Qualifax observations:

UL observations:

CAO observations:
- Inconsiticy with the cao points e.g 2001, 2002, error of 0.1%


Process:
Scrapers > main.py
recommendation_model > preprocessors.py

Struggling with clustering, once the data is clustered, then what? Need unsupervised solution.

john nelson -> steve walker -> chris kelly -> web3.0 -> semantic analysis of data; googling "i love x" and "i <3 x" != the future

first_coherence_test.png -> using tokenizer & stemming
    number_of_topics = 6
    words = 10

model output:


second_coherence_test.png -> removed punctuation from fulldesc, using tokenizer & lemmatization
params:
    number_of_topics = 5
    words = 10

model output:
[(0, '0.314*"student" + 0.256*"course" + 0.250*"programme" + 0.217*"skill" + 0.194*"study" + 0.190*"year" + 0.169*"graduate" + 0.145*"module" + 0.142*"research" + 0.141*"career"'), (1, '0.308*"module" + 0.274*"open" + 0.263*"credit" + 0.246*"pathway" + 0.242*"learning" + 0.187*"entry" + -0.182*"course" + 0.169*"ucd" + 0.140*"used" + 0.138*"mature"'), (2, '-0.648*"subject" + -0.203*"group" + -0.189*"year" + -0.166*"double" + -0.163*"15" + -0.148*"credit" + -0.132*"study" + -0.125*"degree" + -0.125*"taken" + 0.123*"course"'), (3, '0.427*"course" + -0.408*"research" + 0.312*"business" + 0.298*"management" + -0.182*"student" + -0.179*"programme" + 0.152*"marketing" + 0.132*"level" + 0.130*"online" + -0.117*"graduate"'), (4, '-0.480*"business" + 0.388*"course" + -0.335*"management" + 0.242*"education" + -0.209*"programme" + 0.186*"care" + 0.158*"level" + 0.125*"subject" + -0.124*"marketing" + 0.109*"health"')]

18-01-22;
Needs a lot of more cleaning before the topics can be more individualised. 

TO_DO - Topic moddeling on documents on a college to college basis, similar courses (use a tiny bit of supervised leanring of just searching title for keywords: "Engineering", "Health")

Testing John Nelsons recommendation, see if info on bookofmodules contradicts info on timetable.ul.ie - integrity_test.py

problem that processing of text using topic modelling - the bain of existence of processing textual data.
e.g. book spider

/investigations/integrity_test_data/notebook.ipynb output:

    diff in bom and tt: 0
    len of merged tt and bom: 864
    modules of no offset: 260
    modules of no offset but incorrect hours: 143
    Of modules with no offset but have conflicting delivery hours (Modules which have equal total hours for the week)
    LAB > Lab: 16.783216783216783
    LEC > Lecture: 13.286713286713287
    TUT > Tutorial: 71.32867132867133

    We see of the 143 modules that preserved their bom hours on the timetables, tutorials have become more popular.




https://trainingmag.com/top-7-ways-artificial-intelligence-is-used-in-education/
Universal access:
    Educational classrooms can become globally available to all students through AI tools, even those that have hearing or visual impairment or speak different languages. With a PowerPoint plugin such as Presentation Translator, students have real-time subtitles for everything the teacher says. This opens up new possibilities for students who need to learn at different levels, who want to learn a subject unavailable in their school, or are sick and absent from schools. AI can break down the silos between traditional grade levels and schools.

Conclusion:
Anyone whoâ€™s informed about global trends across different industries will know that personalization is top of the list. This is due to the advent of artificial intelligence, which is an advantage for the education sector. AI helps teachers up their game, providing them all the information that they need. It also allows teachers to create content that suits their students best while ensuring personalized learning. It automates tasks, so teachers have more time to do more teaching and impact the students better.


investigate pylint, creation of uml diagrams.

tue 8 feb 2022:
objective for now: get wikipedia data, subcategorise this and use the titles of academic disciplines and use that to categorise our courses.
once we categorise the wiki disciplines, we can use the wiki trained model to categorise.
but first lets look at the course names, see if we can deduct any categorisation from here. topic_modelling/coursename_cat.nbpy

wed 16 2022:
was testing inconsistent prerequisits on book of modules and found:
https://bookofmodules.ul.ie/Default.aspx?ModuleCodeParameter=|CE4704| does not exist when
https://bookofmodules.ul.ie/Default.aspx?ModuleCodeParameter=|CE4706| lists it as prerequisit
also CE4704 does not exist in master data

just realised I can do this:
filter by year and click arrow to go through modules.
can get any module ever all the way back to 1994/95

a lot of documentation always refers to numerical analysis, simple searches like for removing outliers returns this

2 march 22
idea of what is wanted: similarity.py
find term frequency, idea of how rare we wish to make key words
test on small data set to find the big errors first to get an idea of what is optimal and 
what variables to input so we can tweak easily.
The idea is to create a lot of functions on which would be useful in preparing data, through doing text analysis in order to 
have the available tools that would be useful in categorising corpus' using the textual data of course titles first.
short string analysis may not be so useful when it comes to larger text data, but starting with course titles we can work our way up.
some useful functions would be a term frequency index:

corpus = [
    'Bachelor of Arts in Applied Languages',
    'Bachelor of Arts in Contemporary Dance',
    'Bachelor of Arts in Criminal Justice',
    'Bachelor of Arts in European Studies',
    'Bachelor of Arts in International Business',
    'Bachelor of Arts in International Insurance and European Studies',
    'Bachelor of Arts in Irish Dance',
    'Bachelor of Arts in Irish Music',
    'Bachelor of Arts in Journalism and New Media',
    'Bachelor of Arts in Law and Accounting',
    'Bachelor of Arts in Psychology and Sociology',
    'Bachelor of Arts in Voice',
    'Bachelor of Arts in World Music',
    'Bachelor of Business Studies',
    'Bachelor of Business Studies with French',
    'Bachelor of Business Studies with German',
    'Bachelor of Business Studies with Japanese',
    'Bachelor of Engineering in Aeronautical Engineering',
    'Bachelor of Engineering in Biomedical Engineering',
    'Bachelor of Engineering in Chemical and Biochemical Engineering',
    'Bachelor of Engineering in Civil Engineering',
    ]

a dictionary of integers which are the number of instances and are keyworded by words present in a text corpus.
{'bachelor': 21, 'of': 21, 'arts': 13, 'in': 17, 'applied': 1, 'languages': 1, 'contemporary': 1, 'dance': 2, 'criminal': 1, 'justice': 1, 'european': 2, 'studies': 6, 'international': 2, 'business': 5, 'insurance': 1, 'and': 5, 'irish': 2, 'music': 2, 'journalism': 1, 'new': 1, 'media': 1, 'law': 1, 'accounting': 1, 'psychology': 1, 'sociology': 1, 'voice': 1, 'world': 1, 'with': 3, 'french': 1, 'german': 1, 'japanese': 1, 'engineering': 8, 'aeronautical': 1, 'biomedical': 1, 'chemical': 1, 'biochemical': 1, 'civil': 1}

bachelor :       15.909090909090908
of :     15.909090909090908
arts :   9.848484848484848
in :     12.878787878787879
applied :        0.7575757575757576
languages :      0.7575757575757576
contemporary :   0.7575757575757576
dance :          1.5151515151515151
criminal :       0.7575757575757576
justice :        0.7575757575757576
european :       1.5151515151515151
studies :        4.545454545454546
international :          1.5151515151515151
business :       3.787878787878788
insurance :      0.7575757575757576
and :    3.787878787878788
irish :          1.5151515151515151
music :          1.5151515151515151
journalism :     0.7575757575757576
new :    0.7575757575757576
media :          0.7575757575757576
law :    0.7575757575757576
accounting :     0.7575757575757576
psychology :     0.7575757575757576
sociology :      0.7575757575757576
voice :          0.7575757575757576
world :          0.7575757575757576
with :   2.272727272727273
french :         0.7575757575757576
german :         0.7575757575757576
japanese :       0.7575757575757576
engineering :    6.0606060606060606
aeronautical :   0.7575757575757576
biomedical :     0.7575757575757576
chemical :       0.7575757575757576
biochemical :    0.7575757575757576
civil :          0.7575757575757576

looking at this output, we see that bachelor takes up 15% of the words present in the presented corpus.
We will need our algorthim to identify rare terms itself, like engineering, aeronautical, chemical.
We could easily say "find the lowest n terms", but terms like engineering, are quite far away from terms like aeronautical and chemical in frequency, although it is just as important in understanding the context.
As "I study chemicals" and "I study chemical engineering", should not be both summarised as a chemical documents as the latter would also be an engineering document.

We need to look at the larger picture, what terms and phrases are common among all engineering documents, same for chemical, mathematical, etc.
after we got a big picture view, we can distinguish generally what the document is representing, we can go from there to narrow it down and even produce additional distinguishing keywords or topics

continuing from the output above, we see non-useful words like with, in, of, bachelor are all very saturated. We will build our own function to gather stopwords, and remove them based on 
how many times they appear per string, and over all.